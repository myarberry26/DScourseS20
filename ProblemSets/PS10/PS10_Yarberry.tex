\documentclass{article}
\usepackage[utf8]{inputenc}

\title{P10 Yarberry}
\author{Megan N. Yarberry }
\date{April 7, 2020}

\begin{document}

\maketitle

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
  & Learners & Optimal Values &\\ [1ex] 
 \hline\hline
 1 & Trees & 7120 &\\ [1ex]
 \hline
 2 & Logistic Regression & 7304 &\\ [1ex]
 \hline
 3 & Neural Network & 7223  &\\ [1ex]
 \hline
 4 & Naive Bayes & 7120 & \\ [1ex]
 \hline
 5 & kNN & 7024 &\\ [1ex]
 \hline 
 6 & SVM & 7007 &\\ [1ex] 
 \hline
\end{tabular}
\end{center}


\textbf{How does each algorithmâ€™s out-of-sample performance compare
with each of the other algorithms?}
\begin{itemize}
\item As the simulations become more and more complex as you make your way down the list and the longer they take to load the better the out of sample becomes from the the sample left making it a better predictor of the data. 
\end{itemize}

\end{document}
